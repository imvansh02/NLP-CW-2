{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AUzW10SdGtnh"
   },
   "source": [
    "# **Loading The Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load training data\n",
    "train_df = pd.read_csv(\"olid-training-v1.0.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Show data preview\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xmoy411DG1dX"
   },
   "source": [
    "# **EDA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Check label distribution\n",
    "label_counts = train_df['subtask_a'].value_counts()\n",
    "sns.barplot(x=label_counts.index, y=label_counts.values)\n",
    "plt.title(\"Label Distribution (NOT vs OFF)\")\n",
    "plt.xlabel(\"Label\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "# Print basic stats\n",
    "print(train_df['tweet'].str.len().describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lbdLkl_nG3wR"
   },
   "source": [
    "# **Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import emoji\n",
    "\n",
    "# Basic cleaning functions\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"@USER\", \"\", text)\n",
    "    text = re.sub(r\"URL\", \"\", text)\n",
    "    text = emoji.demojize(text, delimiters=(\" \", \" \"))\n",
    "    text = re.sub(r\"http\\S+|www.\\S+\", \"\", text)\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing\n",
    "train_df['clean_tweet'] = train_df['tweet'].apply(clean_text)\n",
    "\n",
    "# Label encoding\n",
    "label_map = {'NOT': 0, 'OFF': 1}\n",
    "train_df['label'] = train_df['subtask_a'].map(label_map)\n",
    "\n",
    "# Show cleaned sample\n",
    "train_df[['tweet', 'clean_tweet', 'label']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "npG-OByjltSP"
   },
   "source": [
    "# **BERT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBertForSequenceClassification, BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "# Set max token length\n",
    "MAX_LEN = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the cleaned tweets\n",
    "def tokenize_data(texts, labels):\n",
    "    encodings = tokenizer(\n",
    "        texts.tolist(),\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=MAX_LEN,\n",
    "        return_tensors=\"tf\"\n",
    "    )\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((\n",
    "        dict(encodings),\n",
    "        tf.convert_to_tensor(labels)\n",
    "    ))\n",
    "    return dataset\n",
    "\n",
    "# Split into train/val\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_df[\"clean_tweet\"],\n",
    "    train_df[\"label\"],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=train_df[\"label\"]\n",
    ")\n",
    "\n",
    "# Tokenised datasets\n",
    "train_dataset = tokenize_data(X_train, y_train).shuffle(1024).batch(32)\n",
    "val_dataset = tokenize_data(X_val, y_val).batch(32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERT for binary classification\n",
    "model_bert = TFBertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metrics = [\"accuracy\"]\n",
    "\n",
    "model_bert.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_bert.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=5, # You can increase this for better performance\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "y_preds = model_bert.predict(val_dataset)[\"logits\"]\n",
    "y_pred_classes = np.argmax(y_preds, axis=1)\n",
    "\n",
    "# Print metrics\n",
    "print(classification_report(y_val, y_pred_classes, target_names=[\"NOT\", \"OFF\"]))\n",
    "print(\"Macro F1 Score:\", f1_score(y_val, y_pred_classes, average='macro'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zPX_NVYJrGVF"
   },
   "source": [
    "# **RoBERTa**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, TFRobertaForSequenceClassification\n",
    "\n",
    "# Load tokenizer and model\n",
    "roberta_tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "roberta_model = TFRobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenisation function\n",
    "def tokenize_roberta(texts, labels):\n",
    "    encodings = roberta_tokenizer(\n",
    "        texts.tolist(),\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=128,\n",
    "        return_tensors=\"tf\"\n",
    "    )\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((\n",
    "        dict(encodings),\n",
    "        tf.convert_to_tensor(labels)\n",
    "    ))\n",
    "    return dataset\n",
    "\n",
    "# Create train and validation datasets\n",
    "train_dataset_roberta = tokenize_roberta(X_train, y_train).shuffle(1024).batch(32)\n",
    "val_dataset_roberta = tokenize_roberta(X_val, y_val).batch(32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metrics = [\"accuracy\"]\n",
    "\n",
    "roberta_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_roberta = roberta_model.fit(\n",
    "    train_dataset_roberta,\n",
    "    validation_data=val_dataset_roberta,\n",
    "    epochs=3  # Adjust as needed\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "y_pred_roberta_logits = roberta_model.predict(val_dataset_roberta)[\"logits\"]\n",
    "y_pred_roberta = np.argmax(y_pred_roberta_logits, axis=1)\n",
    "\n",
    "# Metrics\n",
    "print(classification_report(y_val, y_pred_roberta, target_names=[\"NOT\", \"OFF\"]))\n",
    "print(\"Macro F1 Score:\", f1_score(y_val, y_pred_roberta, average='macro'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3WDNOMNCvPQQ"
   },
   "source": [
    "# **BiLSTM with GloVe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenise the cleaned tweets\n",
    "tokenizer_lstm = Tokenizer(num_words=20000, oov_token=\"<OOV>\")\n",
    "tokenizer_lstm.fit_on_texts(train_df[\"clean_tweet\"])\n",
    "\n",
    "# Convert text to sequences\n",
    "X_seq = tokenizer_lstm.texts_to_sequences(train_df[\"clean_tweet\"])\n",
    "X_padded = pad_sequences(X_seq, maxlen=128, padding='post', truncating='post')\n",
    "\n",
    "# Labels\n",
    "y_lstm = train_df[\"label\"].values\n",
    "\n",
    "# Split data\n",
    "X_train_lstm, X_val_lstm, y_train_lstm, y_val_lstm = train_test_split(\n",
    "    X_padded, y_lstm, test_size=0.2, random_state=42, stratify=y_lstm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download GloVe (if not already)\n",
    "!wget -q http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip -q glove.6B.zip\n",
    "\n",
    "# Load 100d embeddings\n",
    "embedding_index = {}\n",
    "with open(\"glove.6B.100d.txt\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_index[word] = vector\n",
    "\n",
    "# Create embedding matrix\n",
    "embedding_dim = 100\n",
    "word_index = tokenizer_lstm.word_index\n",
    "embedding_matrix = np.zeros((20000, embedding_dim))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i < 20000 and word in embedding_index:\n",
    "        embedding_matrix[i] = embedding_index[word]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm = Sequential([\n",
    "    Embedding(input_dim=20000, output_dim=embedding_dim,\n",
    "              weights=[embedding_matrix], input_length=128, trainable=False),\n",
    "    Bidirectional(LSTM(64, return_sequences=True)),\n",
    "    Dropout(0.3),\n",
    "    Bidirectional(LSTM(32)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(2, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm.compile(loss='sparse_categorical_crossentropy',\n",
    "                   optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history_lstm = model_lstm.fit(\n",
    "    X_train_lstm, y_train_lstm,\n",
    "    validation_data=(X_val_lstm, y_val_lstm),\n",
    "    epochs=5, batch_size=32\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lstm = model_lstm.predict(X_val_lstm)\n",
    "y_pred_labels_lstm = np.argmax(y_pred_lstm, axis=1)\n",
    "\n",
    "print(classification_report(y_val_lstm, y_pred_labels_lstm, target_names=[\"NOT\", \"OFF\"]))\n",
    "print(\"Macro F1 Score:\", f1_score(y_val_lstm, y_pred_labels_lstm, average='macro'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f3u5dXrR0EBF"
   },
   "source": [
    "# **Ensemble Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all predictions are class labels\n",
    "if y_preds.ndim > 1:\n",
    "    y_pred_bert = np.argmax(y_preds, axis=1)\n",
    "\n",
    "if y_pred_roberta.ndim > 1:\n",
    "    y_pred_roberta = np.argmax(y_pred_roberta, axis=1)\n",
    "\n",
    "if y_pred_labels_lstm.ndim > 1:\n",
    "    y_pred_labels_lstm = np.argmax(y_pred_labels_lstm, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "\n",
    "# Stack predictions\n",
    "stacked_preds = np.vstack([y_pred_bert, y_pred_roberta, y_pred_labels_lstm])\n",
    "\n",
    "# Mode across models (axis=0 → column-wise majority)\n",
    "ensemble_preds = stats.mode(stacked_preds, axis=0, keepdims=False)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔮 Ensemble Model Performance (Majority Voting):\")\n",
    "print(classification_report(y_val, ensemble_preds, target_names=[\"NOT\", \"OFF\"]))\n",
    "print(\"Macro F1 Score:\", f1_score(y_val, ensemble_preds, average='macro'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\"BERT\", \"RoBERTa\", \"BiLSTM\", \"Ensemble\"]\n",
    "f1_scores = [\n",
    "    f1_score(y_val, y_pred_bert, average='macro'),\n",
    "    f1_score(y_val, y_pred_roberta, average='macro'),\n",
    "    f1_score(y_val, y_pred_labels_lstm, average='macro'),\n",
    "    f1_score(y_val, ensemble_preds, average='macro'),\n",
    "]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.bar(model_names, f1_scores, color='skyblue')\n",
    "plt.title(\"Macro F1 Score Comparison\")\n",
    "plt.ylabel(\"Macro F1\")\n",
    "plt.ylim(0, 1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7o-F6jp1OZ5"
   },
   "source": [
    "# **Ensemble Predictions on testset-levela.tsv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test set\n",
    "test_df = pd.read_csv(\"testset-levela.tsv\", sep=\"\\t\", names=[\"id\", \"tweet\"], skiprows=1)\n",
    "\n",
    "# Apply the same cleaning used during training\n",
    "test_df[\"clean_tweet\"] = test_df[\"tweet\"].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_inputs = tokenizer(\n",
    "    test_df[\"clean_tweet\"].tolist(),\n",
    "    padding=True, truncation=True, max_length=128, return_tensors=\"tf\"\n",
    ")\n",
    "bert_logits = model_bert.predict(bert_inputs)[\"logits\"]\n",
    "bert_preds = np.argmax(bert_logits, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_inputs = roberta_tokenizer(\n",
    "    test_df[\"clean_tweet\"].tolist(),\n",
    "    padding=True, truncation=True, max_length=128, return_tensors=\"tf\"\n",
    ")\n",
    "roberta_logits = roberta_model.predict(roberta_inputs)[\"logits\"]\n",
    "roberta_preds = np.argmax(roberta_logits, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_sequences = tokenizer_lstm.texts_to_sequences(test_df[\"clean_tweet\"])\n",
    "lstm_padded = pad_sequences(lstm_sequences, maxlen=128, padding='post', truncating='post')\n",
    "lstm_probs = model_lstm.predict(lstm_padded)\n",
    "lstm_preds = np.argmax(lstm_probs, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mode\n",
    "\n",
    "# Stack and vote\n",
    "stacked_test_preds = np.vstack([bert_preds, roberta_preds, lstm_preds])\n",
    "ensemble_test_preds = mode(stacked_test_preds, axis=0, keepdims=False)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert back to original label names\n",
    "id_to_label = {0: \"NOT\", 1: \"OFF\"}\n",
    "test_df[\"label\"] = [id_to_label[p] for p in ensemble_test_preds]\n",
    "\n",
    "# Save to CSV\n",
    "test_df[[\"id\", \"label\"]].to_csv(\"final_predictions.csv\", index=False)\n",
    "\n",
    "print(\"✅ Ensemble predictions saved to final_predictions.csv!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TNXFd7J21wmE"
   },
   "source": [
    "# **Evaluation on test-set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load predictions\n",
    "preds_df = pd.read_csv(\"final_predictions.csv\")\n",
    "\n",
    "# Load gold labels (no header)\n",
    "gold_df = pd.read_csv(\"labels-levela.csv\", names=[\"id\", \"gold_label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge on ID\n",
    "merged_df = preds_df.merge(gold_df, on=\"id\")\n",
    "\n",
    "# Encode labels for comparison\n",
    "label_to_id = {\"NOT\": 0, \"OFF\": 1}\n",
    "y_true = merged_df[\"gold_label\"].map(label_to_id).values\n",
    "y_pred = merged_df[\"label\"].map(label_to_id).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=[\"NOT\", \"OFF\"]))\n",
    "print(\"Macro F1 Score :\", f1_score(y_true, y_pred, average='macro'))\n",
    "\n",
    "# Optional: confusion matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\", xticklabels=[\"NOT\", \"OFF\"], yticklabels=[\"NOT\", \"OFF\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix - Test Set (Ensemble)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
